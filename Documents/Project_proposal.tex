\documentclass{article} % For LaTeX2e
\usepackage{iclr2022_conference,times}
% Optional math commands from https://github.com/goodfeli/dlbook_notation.
\input{math_commands.tex}

%######## APS360: Uncomment your submission name
\newcommand{\apsname}{Project Proposal}
%\newcommand{\apsname}{Progress Report}
%\newcommand{\apsname}{Final Report}

%######## APS360: Put your Group Number here
\newcommand{\gpnumber}{35}

\usepackage{hyperref}
\usepackage{url}
\usepackage{graphicx}

%######## APS360: Put your project Title here
\title{Let's Beat Geometry Dash  \\ 
APS360 Project Proposal}


%######## APS360: Put your names, student IDs and Emails here
\author{Joel Vadakken  \\
Student\# 10010089798\\
\texttt{j.vadakken@mail.utoronto.ca} \\
\And
Skyler Han  \\
Student\# 1009794830 \\
\texttt{hs.han@mail.utoronto.ca} \\
\AND
Ian Lu  \\
Student\# 1009972139 \\
\texttt{i.lu@mail.utoronto.ca} \\
\And
Jaden Dai \\
Student\# 1009972228 \\
\texttt{jaden.dai@mail.utoronto.ca} \\
\AND
}

% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to \LaTeX{} to determine where to break
% the lines. Using \AND forces a linebreak at that point. So, if \LaTeX{}
% puts 3 of 4 authors names on the first line, and the last on the second
% line, try using \AND instead of \And before the third author name.

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

\iclrfinalcopy 
%######## APS360: Document starts here
\begin{document}


\maketitle

\begin{abstract}
Our team's project proposal is to create a bot that can play 
Geometry Dash using machine learning. The bot will be trained
with two architectures, a convoluted neural network so that it
can identify obstacles in the game, and with deep reinforcement
learning so that the bot can learn to make the right choices.
%######## APS360: Do not change the next line. This shows your Main body page count.
----Total Pages: \pageref{last_page}
\end{abstract}



\section{Introduction}

Geometry Dash is a popular platformer video game 
where the player attempts to navigate through 
obstacles in order to reach the end of a level. 
The game features a variety of game modes and 
unique objects.

A video of the first level can be seen here:
\href{https://www.youtube.com/watch?v=bbVEbqU9wPo}{https://www.youtube.com/watch?v=bbVEbqU9wPo} 

The player is controlled by one input which 
determines whether or not the player jumps. 
Our aim is to build an agent that can beat many 
of the game’s levels. This will involve training 
two models. We aim to analyze screen information 
using a CNN architecture to find the positions 
of structures and hazards, then use a deep 
learning reinforcement architecture to determine 
agent behavior.

Our project was inspired by similar work in other
games such as Mario. However, most other games 
simply require you to jump over basic obstacles,
and the proper decisions for progress are usually 
straightforward. Geometry Dash is unique and 
interesting because many levels feature “fakes” 
where jumping objects can trick the player into death.
See Fig. \ref{fig:cycles_false_jump_img} for an example. 

\begin{figure}[!h]
\begin{center}
\includegraphics[width=0.6\textwidth]{Figs/GeoDash_Cycles_False_jump.png}
\end{center}
\caption{A screenshot from the 9th level “Cycles”. 
Normally, interacting with the yellow orbs allows the
players to jump higher and avoid obstacles. In this case, 
however, interacting with any of the yellow orbs will result 
in the player losing. \citep{GD+cycles+screenshot}}
\label{fig:cycles_false_jump_img}
\end{figure}

Furthermore, many community-made levels often feature 
intricate designs that may confuse computer systems, as 
can be seen in Fig. \ref{fig:edge_of_destiny} for example.

\begin{figure}[!h]
\begin{center}
\includegraphics[width=0.6\textwidth]{Figs/GeoDash_Level_ex.png}
\end{center}
\caption{A screenshot from the custom level “Edge of Destiny.” 
Note the intricate details that make it harder to see the hazards. 
\citep{GD+edge_of_destiny}}
\label{fig:edge_of_destiny}
\end{figure}

We believe that a deep-learning approach can enable a computer 
to tackle both these issues through object recognition and avoidance 
of fake objects and routes.

\section{Background and Related Work (4 marks)}

\begin{figure}[!h]
\begin{center}
\includegraphics[width=0.8\textwidth]{Figs/CodeNoodlesScreenshot.png}
\end{center}
\caption{A screenshot of CodeNoodle's visualization of his neural network
from his Youtube video. 
\citep{AI+Learns+to+Play+GD}}
\label{fig:Code_Noodles}
\end{figure}

\cite{AI+Learns+to+Play+GD} created a relatively simple 
Neural Network to play Geometry Dash that takes in 
some information (ie. intersection with  “orb” objects, 
distance to blocks, overhead blocks) and outputs an action.
See Fig \ref{fig:Code_Noodles}.
However, we believe 
that this solution is inadequate as it runs on a simplified 
version of Geometry Dash instead of the official game. The 
distance and position of the obstacles are readily available 
information to the model, which would not be the case in the 
real version. Furthermore, the simplistic neural network 
essentially functions as a decision tree, which could fail 
in some cases to the aforementioned misleading objects.

\begin{figure}[!h]
\begin{center}
\includegraphics[width=0.8\textwidth]{Figs/Trackmania_AI_neural_network.png}
\end{center}
\caption{A screenshot of Yosh's visualization of his neural network
for his trackmania bot. 
\citep{Trackmania+AI}}
\label{fig:Trackmania_AI}
\end{figure}

\cite{Trackmania+AI} created a Youtube video that demonstrated that an AI 
can come to understand complex in-game physics systems and surpass
human-level performance through training. See Fig \ref{fig:Trackmania_AI}.
Trackmania shares a similar overarching gameplay system to Geometry Dash in 
which a player must 
consider various environmental features (ie. track shape, player 
position) in order to decide when and when not to activate various 
inputs. Thus, we believe this video helps demonstrate the potential 
for Deep Learning to work in Geometry Dash.

\cite{Robot+Object+Avoidance+Method} wrote a paper detailing how 
they split up an image for a robot into a bunch of different segments. 
This might be relevant for us as we could split up the images we attain
from Geometry Dash into an array of squares, and try to sample at a
specific framerate.

\begin{figure}[!h]
\begin{center}
\includegraphics[width=1.0\textwidth]{Figs/Q_Learn_Game.png}
\end{center}
\caption{Video game with optimal path shown (left) vs Deep Q Agent Heat Map 
(right). \citep{General+Video+Game+AI:+Learning+from+Screen+Capture}}
\label{fig:Q_Learn_game}
\end{figure}

This paper explicitly uses a combination of CNN and Q-learning network to 
train an AI that can play a variety of games. They utilize CNNs to perform 
feature extraction and determine the best course of action, while the Q-
learning component aims to improve decision-making capabilities. They also 
utilize various pre-processing methods (resizing, etc.). Their work 
culminated in an agent that could effectively traverse some 2D environments.
	
This paper showcases how a combination of CNN and Q-learning can enable an 
agent to plan a path through a level, which is fairly similar to what we are 
attempting to achieve. Their techniques may be a useful reference for our own 
project.

\begin{figure}[!h]
\begin{center}
\includegraphics[width=1.0\textwidth]{Figs/Deepmind_atari.png}
\end{center}
\caption{Screenshots from 5 Atari 2600 games: 
(Left-to-right) Pong, Breakout, Space Invaders, Seaquest, Beam Rider
\citep{Playing+Atari+with+Deep+Reinforcement+Learning}}
\label{fig:Deep_mind_Atari_AI}
\end{figure}

The first trial of applying deep reinforcement learning to train a
video game agent originates from a paper written by
\cite{Playing+Atari+with+Deep+Reinforcement+Learning} at 
DeepMind. They created a program that could play Atari games using
deep reinforcement learning. See Fig. \ref{fig:Deep_mind_Atari_AI}
This paper demonstrates that a convolutional neural network can 
overcome the long training process of reinforcement learning to 
learn successful control policies from raw video data in complex 
RL environments. The network is trained with a variant of the 
Q-learning algorithm, with stochastic gradient descent to update 
the weights. And the game agent surpassed an expert human player 
in every game they implemented.

\cite{DeepMind+Sima} at Deepmind created a model, called SIMA, 
that can take visual observations from a game to carry out an 
instruction by outputting keyboard events. Since we are 
attempting to do something similar in Geometry Dash, it is 
worthwhile to look into their paper. Although the carrying 
out instructions part is not as important, it will be worthwhile
to look into how they analyzed the visual observations from the 
game.

\section{Data Processing (2 marks)}

Our data for our two architectures will both come from the game itself. A CNN 
model will classify the data, and we will use that to create an environment to 
train the RL model. 

In order to identify the types of terrain objects we propose the use of a CNN 
to classify different terrain objects into numerical values in a processed 
tensor that can be passed as an input into the RL model. 

To reduce computational complexity and to preserve the accuracy of the CNN 
classifier, each frame will be grayscale, and the background and each pixel 
will take the value of the mean colour in its associated area such that each 
pixel will consist of one colour value. 

The CNN will be trained on various snapshot frames throughout Geometry Dash 
levels and the corresponding labels for each object terrain in each frame can 
be created using the GD MegaHack v7 mod which identifies the terrain object 
types on a given screen. The CNN will take as an input a single Geometry Dash 
frame, and will output a tensor of the same size with the rgb dimension removed. 
The processed values of the tensor will each be associated with a different 
terrain object in Geometry Dash. Ex: 
\begin{itemize}
\item 0 for air (nothing)
\item 1 for a platform
\item -1 for a spike
\item Some set of integers for portals
\item Some set of integers for orbs
\item Some set of integers for pads
\item Other integers for other game elements
\end{itemize}
The RL model will use the CNN model as it trains in its environment. The tensor 
size of a singular dataset will consist of the number of frames in that geometry 
dash level, the number of pixels on a screen per frame (height x width) and the 
RGB values for each pixel to formulate a tensor of size (number of frames, number 
of pixels to make up a column, number of pixels to make up a row, 3 RGB values). 
The RL model will use this data to output a tensor of size 1 that dictates 
whether 
the bot should jump or not jump. 


\section{Architecture (2 marks)}

\subsection{CNN Model}

\begin{figure}[!htbp]
\begin{center}
\includegraphics[width=1.0\textwidth]{Figs/CNN_sample_structure.png}
\end{center}
\caption{An example of how a CNN neural network might analyze our data.}
\label{fig:CNN_neural_network}
\end{figure}

We will utilize a CNN to analyze the game screen and find the positions of hazards, platforms, and other interactable objects, such as can be seen in Fig \ref{fig:CNN_neural_network}. A CNN is a type of ANN that retains the geometry of the data and performs convulutions  This will leverage the CNN architecture’s ability to understand complex shapes and patterns, and will enable accurate positional information that can later be used by the RL agent.

The CNN will be comprised of a set number of layers and will take in as an
input one frame from the game screen (640x480x3 Tensor), and output 
640x480 array of integer values representing different object types.
 

\subsection*{Reinforcement Learning Model}

\begin{figure}[!htbp]
\begin{center}
\includegraphics[width=1.0\textwidth]{Figs/QLearn_img.png}
\end{center}
\caption{A flowchart that shows how Deep Q learning is used.}
\label{fig:Deep_Q_Learn}
\end{figure}

We will use a similar reinforcement learning model as was developed by 
\cite{Playing+Atari+with+Deep+Reinforcement+Learning}. This will include
the following:

\begin{itemize}
    \item[\textbf{1.}] \textbf{Environment}
    \begin{itemize}
        \item The Geometry Dash game environment needs to be set up so that it 
        can interact with the agent. 
        \item The environment should provide the current state of the game 
        (e.g., position of the player, obstacles) and allow the agent to perform 
        actions (e.g., jump, no jump).
        \item We would need to build the environment with OpenAI Gym to train 
        the agent since we would need to interact with the RL modules.
    \end{itemize}

    \item[\textbf{2.}] \textbf{State Representation}
    \begin{itemize}
        \item We can get the state from the feedback from the CNN by knowing 
        where the obstacle is in the frame and the position of the player.
    \end{itemize}

    \item[\textbf{3.}] \textbf{Action Space}
    \begin{itemize}
        \item The action space \( A \) consists of discrete actions the agent 
        can take. 
        \item In Geometry Dash, this could be simplified to two actions: jump 
        or no jump.
    \end{itemize}

    \item[\textbf{4.}] \textbf{Replay Memory}
    \begin{itemize}
        \item We store the Geometry Dash agent’s experiences at each time-step, 
        \( e_t = (s_t, a_t, r_t, s_{t+1}) \) in a dataset \( D = \{e_1, \ldots, e_N\} \).
        \item These experiences are pooled over many episodes into a replay memory.
        \item We apply Q-learning updates to a small batch of the samples 
        randomly chosen from the dataset.
    \end{itemize}

    \item[\textbf{5.}] \textbf{Reward Function}
    \begin{itemize}
        \item Define a reward function that provides positive rewards for 
        progress (e.g., distance covered, points scored) and negative rewards 
        for failures (e.g., hitting an obstacle).
    \end{itemize}

    \item[\textbf{6.}] \textbf{Training Procedure}
    \begin{itemize}
        \item Initialize the replay memory \( D \) to a fixed capacity.
        \item Initialize the Q-network with random weights.
        \item For each episode:
        \begin{itemize}
            \item Initialize the starting state \( s_1 \).
            \item For each time-step \( t \):
            \begin{itemize}
                \item With probability \( \epsilon \), select a random action \( a_t \). Otherwise, select \( a_t = \arg\max_a Q(s_t, a; \theta) \).
                \item Execute action \( a_t \) in the game and observe reward \( r_t \) and next state \( s_{t+1} \).
                \item Store experience \( e_t = (s_t, a_t, r_t, s_{t+1}) \) in replay memory \( D \).
                \item Sample a random minibatch of experiences \( (s_j, a_j, r_j, s_{j+1}) \) from \( D \).
                \item Compute the target Q-value:
                \[
                y_j = \begin{cases}
                r_j & \text{if } s_{j+1} \text{ is terminal} \\
                r_j + \gamma \max_{a'} Q(s_{j+1}, a'; \theta) & \text{otherwise}
                \end{cases}
                \]
                \item Perform a gradient descent step on the loss \( (y_j - Q(s_j, a_j; \theta))^2 \).
                \item Update the state \( s_t \) to \( s_{t+1} \).
            \end{itemize}
            \item Reduce \( \epsilon \) gradually to reduce exploration over time.
        \end{itemize}
    \end{itemize}
\end{itemize}



\section{Baseline Model (2 marks)}
Describe a simple, baseline model that you will compare your neural
network against. This can be a simple machine learning model (e.g., SVM, Random Forests, etc.), a hand-coded heuristic model (that does not use machine learning), or something else. Expectations for the baseline model will vary from project to project.

\subsection{Baseline Model for CNN}
We will compare our CNN model against a hardcoded system 
that labels the obstacles in the game. Using OpenCV, This system will
be able to identify the obstacles in the game and will 
be able to recognize the player's position. The information 
will then be fed into the baseline model for RL to make decisions.
\subsection{Baseline Model for RL}
We will compare our RL model against a simple decison tree that
will jump based on the feedback from the baseline CNN model.
The bot will jump when OpenCV detects an obstacle in the
game and will not jump when there is no obstacle. The system will 
be able to play the game, but it will not be able to learn from
its mistakes or improve its performance over time.





% Your discussion here

\section{Ethical Considerations (2 marks)}
Geometry Dash has online levels that users can play to earn rewards 
such as diamonds, orbs, and stars. These stars can be used to rank 
the player on the global leaderboard. Developing a bot that can complete 
Geometry Dash with machine learning can result in players gaining an 
unfair advantage over their peers as they climb the leaderboard, and 
result in people playing the game in a way the developers did not intend.

Another ethical consideration could be in the training of the models. 
It is unethical to use the work of others for profit without their 
permission. Levels that are created and published by Geometry Dash 
users online are their own creative works, and it may be unethical to 
use their work to train a model that can be used to generate an income 
without their consent. As a result, our group does not intend to 
commercialize this project.


\section{Project Plan (4 marks)}
% Your conclusion here



\section{Risk Registrar (4 marks)}
The largest potential risk of this project is that we are unable to 
finish. We would like to train a bot that can complete Geometry Dash, 
but this would require that we finish training both the CNN model and 
the RL model in time. If we are unable to finish both, our group would 
just submit the CNN model for our final submission, as the CNN model 
would fulfill the requirements of this project, while the RL model is 
beyond the scope of this course. Although an RL model would be nice, 
we may have to sacrifice it if time does not permit. 

There is also a risk that our group will not even finish the CNN model 
in time. There is a risk that our group will leave deliverables to the 
last minute, resulting in too little time to properly train our model 
and embark on the iterative process of trial and failure that marks all
successful projects. To combat this tendency, we have decided as a team 
to implement many internal deadlines so that our model will have plenty 
of time to train.

Another potential risk of this project could be that our code only works
on one machine, as the course instructor made it clear that the TA grading
our project should be able to run our code. We intend to make the bot 
able to play Geometry Dash, which could require that our bot access the 
keyboard on its host computer, which could involve different libraries 
and processes for different machines and operating systems. Although 
we could try to mitigate this as much as possible by testing on various 
computers and making sure it works on all of our different machines, time
constraints may disallow us from solving this issue in time. If we are 
unable to guarantee that it can run on any machine, we may have to preface 
our final submission with a warning that it only runs on a certain 
operating system. As of right now, our team plans to develop our model 
for Geometry Dash on Linux. 

Lastly, there is the risk that one of our team members is unable to finish
their portion of the project due to an outside situation. Fortunately, 
we have clearly outlined the responsibilities and tasks of each teammate, 
so if a teammate is unable to complete their tasks on time, it will be 
easy for the other teammates to recognize what still needs to be done.


\section{Github Link (1 Mark)}
\href{https://github.com/J-Vadakken/APS360-Project}{https://github.com/J-Vadakken/APS360-Project}



\subsubsection*{Author Contributions}
If you'd like to, you may include  a section for author contributions as is done
in many journals. This is optional and at the discretion of the authors.

\subsubsection*{Acknowledgments}
Use unnumbered third level headings for the acknowledgments. All
acknowledgments, including those to funding agencies, go at the end of the paper.

\label{last_page}

\bibliography{APS360_ref}
\bibliographystyle{iclr2022_conference}

\end{document}
