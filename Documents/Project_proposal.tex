\documentclass{article} % For LaTeX2e
\usepackage{iclr2022_conference,times}
% Optional math commands from https://github.com/goodfeli/dlbook_notation.
\input{math_commands.tex}

%######## APS360: Uncomment your submission name
\newcommand{\apsname}{Project Proposal}
%\newcommand{\apsname}{Progress Report}
%\newcommand{\apsname}{Final Report}

%######## APS360: Put your Group Number here
\newcommand{\gpnumber}{35}

\usepackage{hyperref}
\usepackage{url}
\usepackage{graphicx}

%######## APS360: Put your project Title here
\title{Let's Beat Geometry Dash  \\ 
APS360 Project Proposal}


%######## APS360: Put your names, student IDs and Emails here
\author{Joel Vadakken  \\
Student\# 10010089798\\
\texttt{j.vadakken@mail.utoronto.ca} \\
\And
Author Two  \\
Student\# 1005678901 \\
\texttt{author2@mail.utoronto.ca} \\
\AND
Author Three  \\
Student\# 1005678901 \\
\texttt{author3@mail.utoronto.ca} \\
\And
Author Four \\
Student\# 1005678901 \\
\texttt{author4@mail.utoronto.ca} \\
\AND
}

% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to \LaTeX{} to determine where to break
% the lines. Using \AND forces a linebreak at that point. So, if \LaTeX{}
% puts 3 of 4 authors names on the first line, and the last on the second
% line, try using \AND instead of \And before the third author name.

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

\iclrfinalcopy 
%######## APS360: Document starts here
\begin{document}


\maketitle

\begin{abstract}
Our team's project proposal is to create a bot that can play 
geometry dash using machine learning. The bot will be trained
with two architectures, a convoluted neural network so that it
can identify obstacles in the game, and with deep reinforcement
learning so that the bot can learn to make the right choices.
%######## APS360: Do not change the next line. This shows your Main body page count.
----Total Pages: \pageref{last_page}
\end{abstract}



\section{Introduction}

Geometry Dash is a popular platformer video game 
where the player attempts to navigate through 
obstacles in order to reach the end of a level. 
The game features a variety of game modes and 
unique objects.

A video of the first level can be seen here:
\href{https://www.youtube.com/watch?v=bbVEbqU9wPo}{https://www.youtube.com/watch?v=bbVEbqU9wPo} 

The player is controlled by one input which 
determines whether or not the player jumps. 
Our aim is to build an agent that can beat many 
of the game’s levels. This will involve training 
two models. We aim to analyze screen information 
using a CNN architecture to find the positions 
of structures and hazards, then use a deep 
learning reinforcement architecture to determine 
agent behavior.

Our project was inspired by similar work in other
games such as Mario. However, most other games 
simply require you to jump over basic obstacles,
and the proper decisions for progress are usually 
straightforward. Geometry Dash is unique and 
interesting because many levels feature “fakes” 
where jumping objects can trick the player into death.
See Fig. \ref{fig:cycles_false_jump_img} for an example. 

\begin{figure}[!h]
\begin{center}
\includegraphics[width=0.6\textwidth]{Figs/GeoDash_Cycles_False_jump.png}
\end{center}
\caption{A screenshot from the 9th level “Cycles”. 
Normally, interacting with the yellow orbs allows the
players to jump higher and avoid obstacles. In this case, 
however, interacting with any of the yellow orbs will result 
in the player losing. \citep{GD+cycles+screenshot}}
\label{fig:cycles_false_jump_img}
\end{figure}

Furthermore, many community-made levels often feature 
intricate designs that may confuse computer systems, as 
can be seen in Fig. \ref{fig:edge_of_destiny} for example.

\begin{figure}[!h]
\begin{center}
\includegraphics[width=0.6\textwidth]{Figs/GeoDash_Level_ex.png}
\end{center}
\caption{A screenshot from the custom level “Edge of Destiny.” 
Note the intricate details that make it harder to see the hazards. 
\citep{GD+edge_of_destiny}}
\label{fig:edge_of_destiny}
\end{figure}

We believe that a deep-learning approach can enable a computer 
to tackle both these issues through object recognition and avoidance 
of fake objects and routes.

\section{Background and Related Work (4 marks)}

\begin{figure}[!h]
\begin{center}
\includegraphics[width=0.8\textwidth]{Figs/CodeNoodlesScreenshot.png}
\end{center}
\caption{A screenshot of CodeNoodle's visualization of his neural network
from his Youtube video. 
\citep{AI+Learns+to+Play+GD}}
\label{fig:Code_Noodles}
\end{figure}

\cite{AI+Learns+to+Play+GD} created a relatively simple 
Neural Network to play Geometry Dash that takes in 
some information (ie. intersection with  “orb” objects, 
distance to blocks, overhead blocks) and outputs an action.
See Fig \ref{fig:Code_Noodles}.
However, we believe 
that this solution is inadequate as it runs on a simplified 
version of Geometry Dash instead of the official game. The 
distance and position of the obstacles are readily available 
information to the model, which would not be the case in the 
real version. Furthermore, the simplistic neural network 
essentially functions as a decision tree, which could fail 
in some cases to the aforementioned misleading objects.

\begin{figure}[!h]
\begin{center}
\includegraphics[width=0.8\textwidth]{Figs/Trackmania_AI_neural_network.png}
\end{center}
\caption{A screenshot of Yosh's visualization of his neural network
for his trackmania bot. 
\citep{Trackmania+AI}}
\label{fig:Trackmania_AI}
\end{figure}

\cite{Trackmania+AI} created a Youtube video that demonstrated that an AI 
can come to understand complex in-game physics systems and surpass
human-level performance through training. See Fig \ref{fig:Trackmania_AI}.
Trackmania shares a similar overarching gameplay system to Geometry Dash in 
which a player must 
consider various environmental features (ie. track shape, player 
position) in order to decide when and when not to activate various 
inputs. Thus, we believe this video helps demonstrate the potential 
for Deep Learning to work in geometry dash.

\cite{Robot+Object+Avoidance+Method} wrote a paper detailing how 
they split up an image for a robot into a bunch of different segments. 
This might be relevant for us as we could split up the images we attain
from Geometry Dash into an array of squares, and try to sample at a
specific framerate.

\begin{figure}[!h]
\begin{center}
\includegraphics[width=1.0\textwidth]{Figs/Deepmind_atari.png}
\end{center}
\caption{Screenshots from 5 Atari 2600 games: 
(Left-to-right) Pong, Breakout, Space Invaders, Seaquest, Beam Rider
\citep{Trackmania+AI}}
\label{fig:Deep_mind_Atari_AI}
\end{figure}

The first trial of applying deep reinforcement learning to train a
video game agent originates from a paper written by
\cite{Playing+Atari+with+Deep+Reinforcement+Learning} at 
DeepMind. They created a program that could play Atari games using
deep reinforcement learning. See Fig. \ref{fig:Deep_mind_Atari_AI}
This paper demonstrates that a convolutional neural network can 
overcome the long training process of reinforcement learning to 
learn successful control policies from raw video data in complex 
RL environments. The network is trained with a variant of the 
Q-learning algorithm, with stochastic gradient descent to update 
the weights. And the game agent surpassed an expert human player 
in every game they implemented.

\cite{DeepMind+Sima} at Deepmind created a model, called SIMA, 
that can take visual observations from a game to carry out an 
instruction by outputting keyboard events. Since we are 
attempting to do something similar in Geometry Dash, it is 
worthwhile to look into their paper. Although the carrying 
out instructions part is not as important, it will be worthwhile
to look into how they analyzed the visual observations from the 
game.

\section{Data Processing (2 marks)}
% Your methods here

\section{Architecture (2 marks)}
% Your results here
\subsection*{Reinforcement Learning Model}

\begin{itemize}
    \item[\textbf{1.}] \textbf{Environment}
    \begin{itemize}
        \item The Geometry Dash game environment needs to be set up so that it can interact with the agent. 
        \item The environment should provide the current state of the game (e.g., position of the player, obstacles) and allow the agent to perform actions (e.g., jump, no jump).
        \item We would need to build the environment with OpenAI Gym to train the agent since we would need to interact with the RL modules.
    \end{itemize}

    \item[\textbf{2.}] \textbf{State Representation}
    \begin{itemize}
        \item We can get the state from the feedback from the CNN by knowing where the obstacle is in the frame and the position of the player.
    \end{itemize}

    \item[\textbf{3.}] \textbf{Action Space}
    \begin{itemize}
        \item The action space \( A \) consists of discrete actions the agent can take. 
        \item In Geometry Dash, this could be simplified to two actions: jump or no jump.
    \end{itemize}

    \item[\textbf{4.}] \textbf{Replay Memory}
    \begin{itemize}
        \item We store the Geometry Dash agent’s experiences at each time-step, \( e_t = (s_t, a_t, r_t, s_{t+1}) \) in a dataset \( D = \{e_1, \ldots, e_N\} \).
        \item These experiences are pooled over many episodes into a replay memory.
        \item We apply Q-learning updates to a small batch of the samples randomly chosen from the dataset.
    \end{itemize}

    \item[\textbf{5.}] \textbf{Reward Function}
    \begin{itemize}
        \item Define a reward function that provides positive rewards for progress (e.g., distance covered, points scored) and negative rewards for failures (e.g., hitting an obstacle).
    \end{itemize}

    \item[\textbf{6.}] \textbf{Training Procedure}
    \begin{itemize}
        \item Initialize the replay memory \( D \) to a fixed capacity.
        \item Initialize the Q-network with random weights.
        \item For each episode:
        \begin{itemize}
            \item Initialize the starting state \( s_1 \).
            \item For each time-step \( t \):
            \begin{itemize}
                \item With probability \( \epsilon \), select a random action \( a_t \). Otherwise, select \( a_t = \arg\max_a Q(s_t, a; \theta) \).
                \item Execute action \( a_t \) in the game and observe reward \( r_t \) and next state \( s_{t+1} \).
                \item Store experience \( e_t = (s_t, a_t, r_t, s_{t+1}) \) in replay memory \( D \).
                \item Sample a random minibatch of experiences \( (s_j, a_j, r_j, s_{j+1}) \) from \( D \).
                \item Compute the target Q-value:
                \[
                y_j = \begin{cases}
                r_j & \text{if } s_{j+1} \text{ is terminal} \\
                r_j + \gamma \max_{a'} Q(s_{j+1}, a'; \theta) & \text{otherwise}
                \end{cases}
                \]
                \item Perform a gradient descent step on the loss \( (y_j - Q(s_j, a_j; \theta))^2 \).
                \item Update the state \( s_t \) to \( s_{t+1} \).
            \end{itemize}
            \item Reduce \( \epsilon \) gradually to reduce exploration over time.
        \end{itemize}
    \end{itemize}
\end{itemize}



\section{Baseline Model (2 marks)}

% Your discussion here

\section{Ethical Considerations (2 marks)}
Geometry Dash has online levels that users can play to earn rewards 
such as diamonds, orbs, and stars. These stars can be used to rank 
the player on the global leaderboard. Developing a bot that can complete 
Geometry Dash with machine learning can result in players gaining an 
unfair advantage over their peers as they climb the leaderboard, and 
result in people playing the game in a way the developers did not intend.

Another ethical consideration could be in the training of the models. 
It is unethical to use the work of others for profit without their 
permission. Levels that are created and published by Geometry Dash 
users online are their own creative works, and it may be unethical to 
use their work to train a model that can be used to generate an income 
without their consent. As a result, our group does not intend to 
commercialize this project.


\section{Project Plan (4 marks)}
% Your conclusion here

\section{Risk Registrar (4 marks)}
The largest potential risk of this project is that we are unable to 
finish. We would like to train a bot that can complete Geometry Dash, 
but this would require that we finish training both the CNN model and 
the RL model in time. If we are unable to finish both, our group would 
just submit the CNN model for our final submission, as the CNN model 
would fulfill the requirements of this project, while the RL model is 
beyond the scope of this course. Although an RL model would be nice, 
we may have to sacrifice it if time does not permit. 

There is also a risk that our group will not even finish the CNN model 
in time. There is a risk that our group will leave deliverables to the 
last minute, resulting in too little time to properly train our model 
and embark on the iterative process of trial and failure that marks all
successful projects. To combat this tendency, we have decided as a team 
to implement many internal deadlines so that our model will have plenty 
of time to train.

Another potential risk of this project could be that our code only works
on one machine, as the course instructor made it clear that the TA grading
our project should be able to run our code. We intend to make the bot 
able to play Geometry Dash, which could require that our bot access the 
keyboard on its host computer, which could involve different libraries 
and processes for different machines and operating systems. Although 
we could try to mitigate this as much as possible by testing on various 
computers and making sure it works on all of our different machines, time
constraints may disallow us from solving this issue in time. If we are 
unable to guarantee that it can run on any machine, we may have to preface 
our final submission with a warning that it only runs on a certain 
operating system. As of right now, our team plans to develop our model 
for Geometry Dash on Linux. 

Lastly, there is the risk that one of our team members is unable to finish
their portion of the project due to an outside situation. Fortunately, 
we have clearly outlined the responsibilities and tasks of each teammate, 
so if a teammate is unable to complete their tasks on time, it will be 
easy for the other teammates to recognize what still needs to be done.


\section{Github Link (1 Mark)}
https://github.com/J-Vadakken/APS360-Project



\subsubsection*{Author Contributions}
If you'd like to, you may include  a section for author contributions as is done
in many journals. This is optional and at the discretion of the authors.

\subsubsection*{Acknowledgments}
Use unnumbered third level headings for the acknowledgments. All
acknowledgments, including those to funding agencies, go at the end of the paper.

\label{last_page}

\bibliography{APS360_ref}
\bibliographystyle{iclr2022_conference}

\end{document}
